# Предсказательная модель данных для прогнозирования метрик сервисов

**Цель**: понимать и предсказывать значение метрик на горизонте определенного периода (по умолчанию 180 дней)

## Модель данных и типы метрик - Features

**HTTP**

`time` - временная метка сбора метрик

`service_name` - наименование (микро)сервиса

`endpoint` - наименование эндпоинта ("ручка")

`request_type` - наименование HTTP метода

`request_count` - количество запросов на момент времени, совершенных после предыдущего сбора метрики

`avg_response_time_ms` - среднее время ответа на запросы, совершенных после предыдущего сбора метрики

`error_rate` - процент ошибок (4ХХ или 5ХХ коды ответов)

---

**Infrastructure**

`time` - временная метка сбора метрик

`team_name` - наименование команды (необязательный на текущий момент параметр)

`service_name` - наименование (микро)сервиса

`metric_type` - тип метрики ресурса (значения: **CPU|Disk|Memory|Network**)

`load_percent` - процент загрузки ресурса

`response_time` - промежуток времени с запроса на состояние и минимальную диагностику ресурса и до ответа на запрос

`recovery_time` - время восстановления ресурса после неполадки или "не ответа"

`measurement_interval_ms` - интервал измерения (по умолчанию 5 секунд для всех - необязательный параметр в текущем случае)

---

**Service health**

`time` - временная метка сбора метрик

`service_name` - наименование (микро)сервиса

`status` - статус сервиса на текущий момент времени (значения: **UP|DOWN**)

`last_incident` - временная метка последнего инцидента на текущем сервисе (в текущем случае необязательное поле)

`uptime_percent` - процент время когда сервис был в статусе UP с последнего момент сбора метрики

---

## Целевые переменные - Targets

**HTTP Metrics**
- `avg_response_time_ms`
- `error_rate`

**Infrastructure Metrics**
- `load_percent`
- `recovery_time_ms`
- `response_time_ms`

**Service Health Metrics**
- `uptime_percent`

---

## Устройство проекта

1. Есть возможность протестировать работу модели локально без подключения к внешним источникам
    - Docker-compose файл поднимает локально сервис с БД и FastAPI клиент, который позволит сгенерировать любое количество метрик
    - Скрипт `rebuild.sh` пересоберет проект и запустит нужные контейнеры. FastAPI будет доступен на урлу - `localhost:8000`
      - Доступное API:
        - `POST /generate-metrics` - генерирует нужное количество метрик. Тело запроса 
        ```
        {
            "days": 100
        }
        ```
2. Все машинное обучение находится в проекте при запуске файла `app/pipeline.py`. Запустите его, используя версию Python:3.10+
    - Внутри скрипта есть закоментированные шаблоны, для анализа данных по конкретному микросервису.
    - Опция **Verbose** при значении True покажет ход выполнения скрипта 
    - Опция **Extended** при значении True помимо модели RNN (основной) также задействует обучение дополнительных моделей Prophet и SARIMA (не забудьте включить опцию Verbose, чтобы увидеть ход выполнения)
      - Пример лога для одного микросервиса в находится в директории *app/logs* в файле *m_temp.log*
    - Вы можете настроить основную модель в файле `app/config/model_config.py`
3. В результате работы модели будут сгенерированы файлы с предсказаниями для каждой метрики, которые будут находиться в директории `data_forecast`
4. В директории *app/slo* находится файл с указанием границ SLO для каждой метрики. Именно с ним будет происходить сравнение предсказанных данных для отправки рекомендацию упрвляющему сервисов.